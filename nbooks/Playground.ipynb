{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "sys.path.append(\"/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity\")\n",
    "from gnn.models.bilstm import BILSTM\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_fs = glob.glob(f\"../logs/ranking_bilstm*\")\n",
    "gcn_fs = glob.glob(f\"../logs/ranking_gcn*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mrr(f_names):\n",
    "    mrrs = []\n",
    "    for f_name in f_names:\n",
    "        \n",
    "        with open(f_name) as f:\n",
    "            for line in f:\n",
    "                if \"mrr\" in line:\n",
    "                    line = line.strip()\n",
    "                    #print(line)\n",
    "                    mrr = line.split(\"mrr:\")[-1].strip()\n",
    "                    mrrs.append(float(mrr))\n",
    "                    #break\n",
    "    return mrrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm = np.array(get_mrr(bilstm_fs))\n",
    "gcn = np.array(get_mrr(gcn_fs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6665"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(bilstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4405"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(gcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.randn(3,4,5)\n",
    "t2 = torch.randn(3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((t1, t2), dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = torch.tensor([0,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3349, -2.3859,  0.9394, -0.5300, -0.0873],\n",
       "         [ 1.1515,  1.3651,  1.2636,  0.3900,  0.2895],\n",
       "         [ 1.5420,  0.1927,  0.4795, -0.5437, -0.8179],\n",
       "         [-2.3833, -0.0490,  0.2781, -0.5772, -0.9853]],\n",
       "\n",
       "        [[-0.7101, -0.6641, -0.5311,  0.0880,  0.2135],\n",
       "         [-1.0279,  0.7321,  1.9088,  0.9577,  2.7588],\n",
       "         [ 0.4350,  1.0682,  1.0461, -0.0229, -0.0103],\n",
       "         [ 0.5641,  0.2650, -2.1782, -0.2950, -0.1581]],\n",
       "\n",
       "        [[ 0.8426,  1.2692, -0.6953, -1.3845,  0.4879],\n",
       "         [ 0.1144,  2.2331, -0.1329, -1.0320,  1.0635],\n",
       "         [-1.9905, -1.8087, -1.2040,  0.0284,  0.5279],\n",
       "         [ 1.1005, -1.9029, -0.0916, -2.5649, -1.6604]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3349, -2.3859,  0.9394, -0.5300, -0.0873],\n",
       "        [ 0.4350,  1.0682,  1.0461, -0.0229, -0.0103],\n",
       "        [ 0.1144,  2.2331, -0.1329, -1.0320,  1.0635]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[torch.arange(3),l,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 5])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [('a', 1), ('c', 3), ('b', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 3), ('b', 2), ('a', 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(l, key=lambda x: x[1], reverse=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.68, 8.68\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:.2f}, {1:.2f}\".format(3.678, 8.678))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create word vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/data/'\n",
    "langs = ['en', 'de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sents = []\n",
    "with open(join(data_dir, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 5, 12])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 5, 6, 9, 2, 0, 9, 3, 2, 8, 3, 8],\n",
       "        [9, 9, 4, 4, 1, 2, 1, 2, 0, 3, 1, 5],\n",
       "        [3, 1, 7, 3, 9, 9, 6, 3, 4, 6, 9, 3],\n",
       "        [4, 6, 8, 1, 4, 1, 8, 6, 4, 5, 5, 0],\n",
       "        [6, 1, 3, 0, 6, 7, 8, 6, 4, 5, 9, 6]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5, 5, 9, 9, 2, 4, 6, 9, 4, 7, 2, 6],\n",
       "        [6, 1, 3, 0, 6, 7, 8, 6, 4, 5, 9, 6],\n",
       "        [2, 9, 9, 2, 1, 1, 4, 4, 1, 0, 7, 5],\n",
       "        [0, 6, 6, 1, 7, 9, 1, 7, 7, 3, 1, 3],\n",
       "        [1, 2, 4, 2, 0, 1, 0, 4, 9, 3, 8, 5],\n",
       "        [5, 1, 4, 8, 0, 9, 5, 4, 4, 3, 6, 9],\n",
       "        [5, 6, 0, 4, 8, 0, 0, 1, 3, 1, 2, 9],\n",
       "        [9, 5, 5, 2, 4, 4, 6, 0, 4, 3, 7, 2],\n",
       "        [4, 7, 1, 0, 6, 4, 2, 1, 0, 8, 3, 9],\n",
       "        [5, 2, 0, 2, 5, 2, 5, 3, 6, 6, 1, 4]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((torch.ones(5), torch.zeros(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat((torch.arange(a\n",
    "                       )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_sizes = [500, 50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/processed/tokens_dataset.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_consec_dict(d):\n",
    "    res = {'UNK': 0}\n",
    "    for k, v in d.items():\n",
    "        res[k] = len(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = {}\n",
    "for v in vocab_sizes:\n",
    "    with open(f'../data/vocabs/iwslt14-en-de-{v}-vocab.txt', 'r') as f:\n",
    "        vocabs[v] = create_consec_dict(dict([line.strip().split() for line in f]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['▁It'],\n",
       "  ['▁can'],\n",
       "  ['▁be'],\n",
       "  ['▁a'],\n",
       "  ['▁very'],\n",
       "  ['▁com', 'p', 'l', 'ic', 'at', 'ed'],\n",
       "  ['▁thing', ','],\n",
       "  ['▁the'],\n",
       "  ['▁o', 'ce', 'an', '.']],\n",
       " [['▁And'],\n",
       "  ['▁it'],\n",
       "  ['▁can'],\n",
       "  ['▁be'],\n",
       "  ['▁a'],\n",
       "  ['▁very'],\n",
       "  ['▁com', 'p', 'l', 'ic', 'at', 'ed'],\n",
       "  ['▁thing', ','],\n",
       "  ['▁what'],\n",
       "  ['▁h', 'um', 'an'],\n",
       "  ['▁he', 'al', 'th'],\n",
       "  ['▁is', '.']],\n",
       " [['▁And'],\n",
       "  ['▁b', 'r', 'ing', 'ing'],\n",
       "  ['▁th', 'o', 'se'],\n",
       "  ['▁t', 'w', 'o'],\n",
       "  ['▁to', 'g', 'et', 'her'],\n",
       "  ['▁m', 'ight'],\n",
       "  ['▁se', 'em'],\n",
       "  ['▁a'],\n",
       "  ['▁very'],\n",
       "  ['▁da', 'un', 't', 'ing'],\n",
       "  ['▁t', 'as', 'k', ','],\n",
       "  ['▁but'],\n",
       "  ['▁what'],\n",
       "  ['▁I', \"'\", 'm'],\n",
       "  ['▁going'],\n",
       "  ['▁to'],\n",
       "  ['▁tr', 'y'],\n",
       "  ['▁to'],\n",
       "  ['▁s', 'ay'],\n",
       "  ['▁is'],\n",
       "  ['▁that'],\n",
       "  ['▁e', 'v', 'en'],\n",
       "  ['▁in'],\n",
       "  ['▁that'],\n",
       "  ['▁com', 'p', 'le', 'x', 'ity', ','],\n",
       "  ['▁there', \"'\", 's'],\n",
       "  ['▁som', 'e'],\n",
       "  ['▁s', 'im', 'p', 'le'],\n",
       "  ['▁them', 'es'],\n",
       "  ['▁that'],\n",
       "  ['▁I'],\n",
       "  ['▁think', ','],\n",
       "  ['▁if'],\n",
       "  ['▁we'],\n",
       "  ['▁und', 'er', 'st', 'and', ','],\n",
       "  ['▁we'],\n",
       "  ['▁can'],\n",
       "  ['▁re', 'ally'],\n",
       "  ['▁mo', 've'],\n",
       "  ['▁for', 'w', 'ar', 'd', '.']],\n",
       " [['▁And'],\n",
       "  ['▁th', 'o', 'se'],\n",
       "  ['▁s', 'im', 'p', 'le'],\n",
       "  ['▁them', 'es'],\n",
       "  ['▁ar', 'en', \"'\", 't'],\n",
       "  ['▁re', 'ally'],\n",
       "  ['▁them', 'es'],\n",
       "  ['▁about'],\n",
       "  ['▁the'],\n",
       "  ['▁com', 'p', 'le', 'x'],\n",
       "  ['▁s', 'c', 'ien', 'ce'],\n",
       "  ['▁of'],\n",
       "  ['▁what', \"'\", 's'],\n",
       "  ['▁going'],\n",
       "  ['▁on', ','],\n",
       "  ['▁but'],\n",
       "  ['▁thing', 's'],\n",
       "  ['▁that'],\n",
       "  ['▁we'],\n",
       "  ['▁all'],\n",
       "  ['▁p', 're', 't', 't', 'y'],\n",
       "  ['▁we', 'll'],\n",
       "  ['▁know', '.']],\n",
       " [['▁And'],\n",
       "  ['▁I', \"'\", 'm'],\n",
       "  ['▁going'],\n",
       "  ['▁to'],\n",
       "  ['▁st', 'art'],\n",
       "  ['▁with'],\n",
       "  ['▁this'],\n",
       "  ['▁one', ':'],\n",
       "  ['▁I', 'f'],\n",
       "  ['▁m', 'om', 'm', 'a'],\n",
       "  ['▁a', 'in', \"'\", 't'],\n",
       "  ['▁ha', 'pp', 'y', ','],\n",
       "  ['▁a', 'in', \"'\", 't'],\n",
       "  ['▁no', 'b', 'od', 'y'],\n",
       "  ['▁ha', 'pp', 'y', '.']]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['en'][500][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 0,\n",
       " ',': 1,\n",
       " '.': 2,\n",
       " 's': 3,\n",
       " 'en': 4,\n",
       " 't': 5,\n",
       " 'e': 6,\n",
       " 'er': 7,\n",
       " '▁the': 8,\n",
       " 'a': 9,\n",
       " 'r': 10,\n",
       " 'o': 11,\n",
       " 'n': 12,\n",
       " '▁in': 13,\n",
       " '▁a': 14,\n",
       " 'l': 15,\n",
       " 'h': 16,\n",
       " 'm': 17,\n",
       " 'es': 18,\n",
       " 'f': 19,\n",
       " 'y': 20,\n",
       " 'p': 21,\n",
       " 'g': 22,\n",
       " 'd': 23,\n",
       " 're': 24,\n",
       " 'ing': 25,\n",
       " 'u': 26,\n",
       " 'i': 27,\n",
       " 'it': 28,\n",
       " '▁to': 29,\n",
       " '▁w': 30,\n",
       " 'b': 31,\n",
       " \"'\": 32,\n",
       " '▁s': 33,\n",
       " 'al': 34,\n",
       " 'k': 35,\n",
       " '▁d': 36,\n",
       " 'ar': 37,\n",
       " '▁of': 38,\n",
       " 'te': 39,\n",
       " 'le': 40,\n",
       " '▁b': 41,\n",
       " 'at': 42,\n",
       " 'in': 43,\n",
       " 'an': 44,\n",
       " '▁and': 45,\n",
       " '▁g': 46,\n",
       " '▁die': 47,\n",
       " 'c': 48,\n",
       " '▁I': 49,\n",
       " 'ch': 50,\n",
       " 'w': 51,\n",
       " '▁f': 52,\n",
       " '▁m': 53,\n",
       " 'ed': 54,\n",
       " '▁h': 55,\n",
       " 'is': 56,\n",
       " '▁und': 57,\n",
       " '▁p': 58,\n",
       " 'ä': 59,\n",
       " '▁l': 60,\n",
       " '▁': 61,\n",
       " 'ten': 62,\n",
       " 'ie': 63,\n",
       " 'on': 64,\n",
       " 'z': 65,\n",
       " '▁c': 66,\n",
       " '▁M': 67,\n",
       " '▁that': 68,\n",
       " '▁be': 69,\n",
       " 'or': 70,\n",
       " 'ur': 71,\n",
       " '▁we': 72,\n",
       " 'el': 73,\n",
       " 'st': 74,\n",
       " 'ra': 75,\n",
       " '▁you': 76,\n",
       " '▁t': 77,\n",
       " '▁der': 78,\n",
       " 'et': 79,\n",
       " '▁ein': 80,\n",
       " 'v': 81,\n",
       " 'as': 82,\n",
       " '▁e': 83,\n",
       " 'ent': 84,\n",
       " '▁W': 85,\n",
       " 'll': 86,\n",
       " 'ter': 87,\n",
       " 'ig': 88,\n",
       " '▁n': 89,\n",
       " '▁wir': 90,\n",
       " '▁is': 91,\n",
       " '▁ist': 92,\n",
       " '▁zu': 93,\n",
       " 'ic': 94,\n",
       " '▁it': 95,\n",
       " '▁A': 96,\n",
       " 'ere': 97,\n",
       " '▁an': 98,\n",
       " 'am': 99,\n",
       " 'om': 100,\n",
       " '▁k': 101,\n",
       " '▁S': 102,\n",
       " '▁das': 103,\n",
       " '▁D': 104,\n",
       " 'ü': 105,\n",
       " 'lich': 106,\n",
       " 'ut': 107,\n",
       " 'em': 108,\n",
       " 'ge': 109,\n",
       " 'ion': 110,\n",
       " 'ol': 111,\n",
       " 'ir': 112,\n",
       " 'il': 113,\n",
       " 'ow': 114,\n",
       " '▁K': 115,\n",
       " '▁re': 116,\n",
       " '▁G': 117,\n",
       " '▁F': 118,\n",
       " 'hr': 119,\n",
       " '▁ich': 120,\n",
       " '▁so': 121,\n",
       " 'us': 122,\n",
       " '▁B': 123,\n",
       " '▁es': 124,\n",
       " 'im': 125,\n",
       " 'ro': 126,\n",
       " '▁was': 127,\n",
       " '-': 128,\n",
       " 'ach': 129,\n",
       " '▁And': 130,\n",
       " 'ct': 131,\n",
       " 'ay': 132,\n",
       " 'gen': 133,\n",
       " '▁T': 134,\n",
       " 'ich': 135,\n",
       " '▁H': 136,\n",
       " '▁me': 137,\n",
       " 'ung': 138,\n",
       " '?': 139,\n",
       " '▁er': 140,\n",
       " 'id': 141,\n",
       " 'un': 142,\n",
       " '▁st': 143,\n",
       " 'ö': 144,\n",
       " '▁E': 145,\n",
       " 'se': 146,\n",
       " 'de': 147,\n",
       " '▁N': 148,\n",
       " 'ver': 149,\n",
       " '▁sie': 150,\n",
       " '▁this': 151,\n",
       " '▁den': 152,\n",
       " 'sch': 153,\n",
       " '▁o': 154,\n",
       " 'ot': 155,\n",
       " 'um': 156,\n",
       " '▁Sie': 157,\n",
       " 'and': 158,\n",
       " '▁se': 159,\n",
       " 'nd': 160,\n",
       " '▁ver': 161,\n",
       " '▁all': 162,\n",
       " '▁--': 163,\n",
       " 'sen': 164,\n",
       " '▁P': 165,\n",
       " '▁\"': 166,\n",
       " '▁th': 167,\n",
       " '▁v': 168,\n",
       " 'be': 169,\n",
       " '▁nicht': 170,\n",
       " '▁von': 171,\n",
       " 'ation': 172,\n",
       " '▁dass': 173,\n",
       " ':': 174,\n",
       " '▁for': 175,\n",
       " '▁wh': 176,\n",
       " '▁Und': 177,\n",
       " 'ind': 178,\n",
       " 'ou': 179,\n",
       " '▁z': 180,\n",
       " '▁R': 181,\n",
       " 'ben': 182,\n",
       " 'ce': 183,\n",
       " 'ke': 184,\n",
       " '▁eine': 185,\n",
       " '▁on': 186,\n",
       " 'ine': 187,\n",
       " '▁ha': 188,\n",
       " '▁auf': 189,\n",
       " 'ier': 190,\n",
       " '▁r': 191,\n",
       " 'ill': 192,\n",
       " '▁do': 193,\n",
       " 'hen': 194,\n",
       " '▁im': 195,\n",
       " 'est': 196,\n",
       " '▁mit': 197,\n",
       " 'ag': 198,\n",
       " 'her': 199,\n",
       " '▁ge': 200,\n",
       " '▁ne': 201,\n",
       " '▁L': 202,\n",
       " '0': 203,\n",
       " 'ess': 204,\n",
       " 'ß': 205,\n",
       " 'ly': 206,\n",
       " '▁J': 207,\n",
       " 'ste': 208,\n",
       " 'if': 209,\n",
       " 'ally': 210,\n",
       " '▁are': 211,\n",
       " 'isch': 212,\n",
       " '▁wie': 213,\n",
       " '▁sch': 214,\n",
       " 'ck': 215,\n",
       " '▁man': 216,\n",
       " '▁uns': 217,\n",
       " 'ul': 218,\n",
       " 'den': 219,\n",
       " '▁war': 220,\n",
       " 'he': 221,\n",
       " '▁with': 222,\n",
       " '▁So': 223,\n",
       " '▁have': 224,\n",
       " '▁C': 225,\n",
       " 'kt': 226,\n",
       " '▁le': 227,\n",
       " 'uch': 228,\n",
       " '▁he': 229,\n",
       " 'men': 230,\n",
       " '▁j': 231,\n",
       " '▁they': 232,\n",
       " 'au': 233,\n",
       " 'os': 234,\n",
       " '▁sich': 235,\n",
       " '▁Th': 236,\n",
       " 'nen': 237,\n",
       " '▁can': 238,\n",
       " 'we': 239,\n",
       " '▁O': 240,\n",
       " '▁mo': 241,\n",
       " '▁Z': 242,\n",
       " 'ers': 243,\n",
       " 'icht': 244,\n",
       " '▁as': 245,\n",
       " 'ab': 246,\n",
       " '▁wor': 247,\n",
       " 'ib': 248,\n",
       " 'x': 249,\n",
       " '▁Ich': 250,\n",
       " 'ant': 251,\n",
       " '▁We': 252,\n",
       " '▁pro': 253,\n",
       " 'ad': 254,\n",
       " 'all': 255,\n",
       " '▁un': 256,\n",
       " '▁what': 257,\n",
       " 'ne': 258,\n",
       " '▁dies': 259,\n",
       " 've': 260,\n",
       " 'iv': 261,\n",
       " 'ort': 262,\n",
       " '▁sind': 263,\n",
       " '▁1': 264,\n",
       " 'und': 265,\n",
       " '▁com': 266,\n",
       " 'uf': 267,\n",
       " '▁sp': 268,\n",
       " '▁at': 269,\n",
       " '▁für': 270,\n",
       " '▁haben': 271,\n",
       " 'ann': 272,\n",
       " 'ies': 273,\n",
       " '▁In': 274,\n",
       " 'pp': 275,\n",
       " 'af': 276,\n",
       " 'ik': 277,\n",
       " 'ould': 278,\n",
       " '▁hat': 279,\n",
       " 'art': 280,\n",
       " 'ze': 281,\n",
       " '▁about': 282,\n",
       " '▁St': 283,\n",
       " '▁ex': 284,\n",
       " 'ern': 285,\n",
       " 'ist': 286,\n",
       " 'op': 287,\n",
       " 'ien': 288,\n",
       " '▁U': 289,\n",
       " '\"': 290,\n",
       " '▁aus': 291,\n",
       " '▁not': 292,\n",
       " '▁V': 293,\n",
       " '▁The': 294,\n",
       " '▁um': 295,\n",
       " 'og': 296,\n",
       " 'iel': 297,\n",
       " '▁Le': 298,\n",
       " '▁ch': 299,\n",
       " '▁über': 300,\n",
       " '▁des': 301,\n",
       " 'ive': 302,\n",
       " 'ahr': 303,\n",
       " 'oll': 304,\n",
       " '▁de': 305,\n",
       " 'der': 306,\n",
       " '▁som': 307,\n",
       " 'hl': 308,\n",
       " '▁or': 309,\n",
       " 'ight': 310,\n",
       " 'end': 311,\n",
       " '▁dem': 312,\n",
       " '▁als': 313,\n",
       " '▁Das': 314,\n",
       " 'ein': 315,\n",
       " 'ür': 316,\n",
       " '.\"': 317,\n",
       " '▁there': 318,\n",
       " '▁Es': 319,\n",
       " '▁Sch': 320,\n",
       " '▁sa': 321,\n",
       " 'qu': 322,\n",
       " 'mer': 323,\n",
       " '▁my': 324,\n",
       " 'ate': 325,\n",
       " '▁Wir': 326,\n",
       " '▁sh': 327,\n",
       " 'igen': 328,\n",
       " 'ht': 329,\n",
       " '▁ma': 330,\n",
       " '▁It': 331,\n",
       " 'od': 332,\n",
       " '▁diese': 333,\n",
       " 'th': 334,\n",
       " '▁con': 335,\n",
       " '▁one': 336,\n",
       " '▁Er': 337,\n",
       " '▁no': 338,\n",
       " 'ld': 339,\n",
       " 'ity': 340,\n",
       " '▁people': 341,\n",
       " 'elt': 342,\n",
       " '▁go': 343,\n",
       " 'ff': 344,\n",
       " 'ure': 345,\n",
       " '▁Be': 346,\n",
       " 'ute': 347,\n",
       " '▁from': 348,\n",
       " '▁like': 349,\n",
       " 'esch': 350,\n",
       " 'tw': 351,\n",
       " 'ide': 352,\n",
       " '▁ar': 353,\n",
       " '▁te': 354,\n",
       " '▁tr': 355,\n",
       " 'ass': 356,\n",
       " '▁wenn': 357,\n",
       " 'ap': 358,\n",
       " '▁also': 359,\n",
       " '▁da': 360,\n",
       " 'ang': 361,\n",
       " '00': 362,\n",
       " 'ber': 363,\n",
       " '▁einen': 364,\n",
       " 'ause': 365,\n",
       " 'ast': 366,\n",
       " '▁out': 367,\n",
       " '▁us': 368,\n",
       " 'ook': 369,\n",
       " 'ensch': 370,\n",
       " '▁get': 371,\n",
       " '▁bec': 372,\n",
       " 'zen': 373,\n",
       " '▁but': 374,\n",
       " '▁Ge': 375,\n",
       " '▁know': 376,\n",
       " 'ieren': 377,\n",
       " '▁our': 378,\n",
       " '▁Ar': 379,\n",
       " '▁ih': 380,\n",
       " '▁y': 381,\n",
       " '▁just': 382,\n",
       " '▁können': 383,\n",
       " '▁Pro': 384,\n",
       " '▁Die': 385,\n",
       " 'ted': 386,\n",
       " 'aus': 387,\n",
       " 'enn': 388,\n",
       " '▁thing': 389,\n",
       " '5': 390,\n",
       " '▁werden': 391,\n",
       " '▁pl': 392,\n",
       " '▁2': 393,\n",
       " 'iert': 394,\n",
       " 'ick': 395,\n",
       " 'j': 396,\n",
       " '▁Y': 397,\n",
       " 'ear': 398,\n",
       " '▁sein': 399,\n",
       " '▁An': 400,\n",
       " 'ild': 401,\n",
       " '▁her': 402,\n",
       " '▁very': 403,\n",
       " 'oug': 404,\n",
       " '▁them': 405,\n",
       " '▁vor': 406,\n",
       " '▁But': 407,\n",
       " '▁these': 408,\n",
       " '▁ent': 409,\n",
       " 'ungen': 410,\n",
       " '▁oder': 411,\n",
       " '▁up': 412,\n",
       " 'per': 413,\n",
       " 'ite': 414,\n",
       " '▁if': 415,\n",
       " 'ah': 416,\n",
       " '▁mach': 417,\n",
       " '▁think': 418,\n",
       " 'ige': 419,\n",
       " '▁kann': 420,\n",
       " 'ound': 421,\n",
       " '▁who': 422,\n",
       " 'our': 423,\n",
       " 'orm': 424,\n",
       " 'ru': 425,\n",
       " '▁ab': 426,\n",
       " 'ank': 427,\n",
       " 'one': 428,\n",
       " '▁–': 429,\n",
       " '▁Ver': 430,\n",
       " '▁going': 431,\n",
       " 'iz': 432,\n",
       " '▁aber': 433,\n",
       " 'ain': 434,\n",
       " '▁by': 435,\n",
       " '▁einer': 436,\n",
       " '▁gen': 437,\n",
       " '▁auch': 438,\n",
       " '▁see': 439,\n",
       " 'ack': 440,\n",
       " '▁Aber': 441,\n",
       " 'llen': 442,\n",
       " 'S': 443,\n",
       " 'ls': 444,\n",
       " '9': 445,\n",
       " 'ust': 446,\n",
       " 'D': 447,\n",
       " '3': 448,\n",
       " 'A': 449,\n",
       " '▁i': 450,\n",
       " 'I': 451,\n",
       " 'ese': 452,\n",
       " 'oo': 453,\n",
       " ';': 454,\n",
       " '4': 455,\n",
       " 'W': 456,\n",
       " '8': 457,\n",
       " '▁u': 458,\n",
       " 'E': 459,\n",
       " '6': 460,\n",
       " 'T': 461,\n",
       " '▁-': 462,\n",
       " '7': 463,\n",
       " '▁fr': 464,\n",
       " '2': 465,\n",
       " 'M': 466,\n",
       " '▁kön': 467,\n",
       " '1': 468,\n",
       " 'P': 469,\n",
       " '▁wer': 470,\n",
       " 'N': 471,\n",
       " 'K': 472,\n",
       " 'ink': 473,\n",
       " 'B': 474,\n",
       " 'über': 475,\n",
       " '!': 476,\n",
       " 'out': 477,\n",
       " 'O': 478,\n",
       " 'C': 479,\n",
       " 'ön': 480,\n",
       " 'H': 481,\n",
       " 'G': 482,\n",
       " 'V': 483,\n",
       " 'R': 484,\n",
       " 'L': 485,\n",
       " 'Ü': 486,\n",
       " 'F': 487,\n",
       " 'Q': 488,\n",
       " 'ith': 489,\n",
       " '[': 490,\n",
       " ']': 491,\n",
       " 'Ä': 492,\n",
       " 'Ö': 493,\n",
       " 'ike': 494,\n",
       " '&': 495,\n",
       " 'J': 496,\n",
       " 'Y': 497,\n",
       " '▁pe': 498,\n",
       " 'U': 499,\n",
       " '%': 500,\n",
       " '/': 501,\n",
       " '—': 502,\n",
       " 'X': 503,\n",
       " 'ople': 504,\n",
       " 'Z': 505,\n",
       " '▁kn': 506,\n",
       " '$': 507,\n",
       " 'q': 508,\n",
       " '«': 509,\n",
       " '»': 510,\n",
       " 'é': 511,\n",
       " '<x>': 512,\n",
       " 'á': 513,\n",
       " '+': 514,\n",
       " '=': 515,\n",
       " '›': 516,\n",
       " '–': 517,\n",
       " '°': 518,\n",
       " ')': 519,\n",
       " '⁄': 520,\n",
       " 'ó': 521,\n",
       " 'í': 522,\n",
       " 'è': 523,\n",
       " '@': 524,\n",
       " '‚': 525,\n",
       " '#': 526,\n",
       " '(': 527,\n",
       " 'β': 528,\n",
       " '^': 529,\n",
       " '_': 530,\n",
       " 'ñ': 531,\n",
       " 'à': 532,\n",
       " 'ã': 533,\n",
       " 'â': 534,\n",
       " 'ç': 535,\n",
       " 'ê': 536,\n",
       " '*': 537,\n",
       " '`': 538,\n",
       " '¥': 539,\n",
       " 'É': 540,\n",
       " 'ø': 541,\n",
       " '\\xad': 542,\n",
       " 'ú': 543,\n",
       " 'ā': 544,\n",
       " '£': 545,\n",
       " '×': 546,\n",
       " 'ë': 547,\n",
       " 'ô': 548,\n",
       " 'œ': 549,\n",
       " '\\\\': 550,\n",
       " '©': 551,\n",
       " '®': 552,\n",
       " 'Å': 553,\n",
       " 'Ç': 554,\n",
       " 'ï': 555,\n",
       " 'ý': 556,\n",
       " 'ć': 557,\n",
       " 'š': 558,\n",
       " '€': 559,\n",
       " '±': 560,\n",
       " 'î': 561,\n",
       " 'Ă': 562,\n",
       " 'ē': 563,\n",
       " 'ō': 564,\n",
       " 'Ť': 565,\n",
       " 'к': 566,\n",
       " '√': 567,\n",
       " 'madeupword0000': 568,\n",
       " 'madeupword0001': 569,\n",
       " 'madeupword0002': 570,\n",
       " 'madeupword0003': 571,\n",
       " 'madeupword0004': 572}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabs[500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm = BILSTM(128, 256, len(vocabs[500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_flat = {lang: [[token for word in sent for token in word] for sent in data_vs[500]] for lang, data_vs in data.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_tokens(values):\n",
    "    size = max(len(v) for v in values)\n",
    "    res = np.zeros((len(values), size), dtype=np.long)\n",
    "    \n",
    "    for i, v in enumerate(values):\n",
    "        res[i][:len(v)] = v\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [1, 2, 0]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collate_tokens([[1,2,3],[1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(data_flat['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "\n",
    "for i, batch_start in enumerate(range(0,l,BATCH_SIZE)):\n",
    "    en = data_flat['en'][batch_start: batch_start + BATCH_SIZE]\n",
    "    de = data_flat['de'][batch_start: batch_start + BATCH_SIZE]\n",
    "    en = collate_tokens([[vocabs[500].get(token, 0) for token in sent] for sent in en])\n",
    "    de = collate_tokens([[vocabs[500].get(token, 0) for token in sent] for sent in de])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.2178e-03, -2.2162e-03,  8.8887e-03,  ..., -5.6052e-03,\n",
       "          -5.9517e-02,  4.3833e-02],\n",
       "         [ 1.2576e-02,  9.2151e-03, -2.9298e-02,  ..., -1.0797e-02,\n",
       "          -7.3682e-02,  2.1178e-02],\n",
       "         [-3.2531e-02, -4.2487e-04, -6.9737e-03,  ...,  2.2516e-04,\n",
       "          -9.3891e-02,  1.6651e-02],\n",
       "         ...,\n",
       "         [ 5.9313e-02, -6.6641e-02,  1.5658e-01,  ..., -6.2147e-02,\n",
       "          -3.3358e-02,  5.0430e-02],\n",
       "         [ 8.9931e-03, -6.8983e-02,  1.2450e-01,  ..., -2.4962e-02,\n",
       "          -7.0752e-02,  4.5551e-02],\n",
       "         [ 2.3369e-02, -6.6386e-02,  7.4812e-02,  ..., -1.0721e-03,\n",
       "          -6.1242e-02,  3.1662e-02]],\n",
       "\n",
       "        [[-1.0074e-02,  1.7195e-02,  6.1169e-03,  ...,  4.0959e-02,\n",
       "          -7.0635e-03,  1.0011e-02],\n",
       "         [ 1.0934e-02,  4.3922e-02,  1.8624e-02,  ...,  5.1706e-02,\n",
       "          -1.6128e-02,  1.1667e-02],\n",
       "         [-8.2074e-03,  1.3459e-02,  3.6670e-02,  ...,  4.2992e-02,\n",
       "           9.2687e-03,  5.8783e-02],\n",
       "         ...,\n",
       "         [ 7.5604e-02, -8.5379e-02,  1.4693e-01,  ...,  3.2998e-02,\n",
       "          -2.5728e-02,  5.0432e-03],\n",
       "         [ 8.3305e-02, -6.5121e-02,  1.4288e-01,  ..., -1.0386e-02,\n",
       "          -2.0152e-02, -1.9230e-02],\n",
       "         [ 6.9755e-02, -6.4814e-02,  1.2262e-01,  ..., -1.4342e-02,\n",
       "          -1.5335e-02,  7.3490e-03]],\n",
       "\n",
       "        [[-3.2729e-03,  1.7228e-02,  3.9772e-02,  ..., -2.7038e-02,\n",
       "           9.8911e-03,  1.2209e-02],\n",
       "         [ 5.5533e-02, -3.2804e-03,  2.1200e-02,  ...,  3.7014e-02,\n",
       "           2.9145e-02,  1.7244e-02],\n",
       "         [ 3.8390e-02,  1.8152e-02, -2.4511e-02,  ...,  5.5717e-02,\n",
       "           4.9619e-02,  5.9078e-02],\n",
       "         ...,\n",
       "         [ 8.5681e-02, -4.0295e-02,  6.9332e-02,  ..., -1.6740e-02,\n",
       "          -4.1426e-02,  8.0864e-02],\n",
       "         [ 6.9961e-02, -3.4063e-02,  1.1945e-01,  ..., -1.2589e-05,\n",
       "          -3.5692e-02,  1.2107e-02],\n",
       "         [ 4.4487e-02, -3.4628e-02,  1.1248e-01,  ...,  4.5912e-03,\n",
       "          -5.4019e-02, -7.2403e-03]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.2328e-02, -5.8735e-02, -9.3094e-03,  ...,  4.8596e-02,\n",
       "          -3.1506e-02,  2.2916e-02],\n",
       "         [-8.7088e-02, -1.8019e-02,  1.7995e-02,  ...,  5.0657e-02,\n",
       "          -8.6698e-03,  1.4547e-02],\n",
       "         [-3.8539e-02,  8.0059e-03,  3.0588e-02,  ...,  5.1754e-02,\n",
       "          -2.6224e-02,  3.3248e-03],\n",
       "         ...,\n",
       "         [ 5.1474e-02, -6.7627e-02,  1.2641e-01,  ...,  6.3590e-03,\n",
       "          -2.4458e-02,  2.1228e-02],\n",
       "         [ 5.7265e-02, -8.5190e-02,  1.1475e-01,  ..., -3.2173e-02,\n",
       "          -2.5483e-02,  3.4871e-02],\n",
       "         [ 4.0661e-02, -7.9022e-02,  1.0316e-01,  ...,  1.0781e-02,\n",
       "          -3.5681e-02,  2.5852e-02]],\n",
       "\n",
       "        [[ 2.9357e-02, -1.4251e-02,  3.2453e-02,  ..., -5.3261e-02,\n",
       "           1.6153e-03,  3.4704e-02],\n",
       "         [ 3.0901e-02, -1.4233e-02,  3.1934e-02,  ..., -3.3704e-02,\n",
       "          -1.1692e-03,  2.6735e-02],\n",
       "         [ 1.9592e-02,  1.8494e-02,  6.9290e-02,  ...,  2.3676e-03,\n",
       "           8.2779e-03,  1.3011e-02],\n",
       "         ...,\n",
       "         [ 4.5667e-02, -8.1199e-03,  1.1764e-01,  ..., -1.6657e-02,\n",
       "           1.8829e-03,  5.1917e-02],\n",
       "         [ 5.0757e-02, -3.3232e-02,  8.6083e-02,  ..., -1.9772e-02,\n",
       "           2.6704e-02,  2.4680e-02],\n",
       "         [ 2.8541e-02, -3.2199e-02,  8.8816e-02,  ...,  3.5789e-04,\n",
       "           1.2205e-02,  1.3302e-02]],\n",
       "\n",
       "        [[-9.2007e-03,  3.7202e-04,  3.5058e-02,  ..., -3.9004e-02,\n",
       "          -3.6797e-05,  2.1581e-02],\n",
       "         [-3.4914e-03,  8.3339e-03,  3.6810e-02,  ..., -4.2748e-02,\n",
       "           1.6759e-02,  1.9649e-03],\n",
       "         [ 1.8019e-02,  2.2660e-02,  5.9194e-02,  ..., -8.2488e-03,\n",
       "           4.9962e-02,  7.2251e-03],\n",
       "         ...,\n",
       "         [ 9.5958e-04, -6.9605e-03, -1.3140e-02,  ...,  1.5627e-02,\n",
       "           1.4291e-02,  1.8297e-02],\n",
       "         [-6.4122e-02, -6.3040e-02, -2.4104e-02,  ...,  5.8295e-02,\n",
       "           3.1102e-02,  2.8146e-02],\n",
       "         [-3.8090e-02, -7.9313e-02,  1.4210e-03,  ...,  3.2899e-02,\n",
       "           9.3980e-03, -1.0842e-02]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm(torch.tensor(en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 49, 418, 415, ...,   0,   0,   0],\n",
       "       [136, 114, 154, ...,   0,   0,   0],\n",
       "       [181, 474, 174, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [236, 427,  76, ...,   0,   0,   0],\n",
       "       [181, 135,  37, ...,   0,   0,   0],\n",
       "       [181, 135,  37, ..., 172,   3,   2]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9295,  1.4196, -0.5438, -1.3986,  1.0015, -0.4257,  0.4278, -1.1540,\n",
       "        -0.2786,  1.5857])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0], dtype=torch.uint8)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(s < 1.) & (s > 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.9295,  1.4196, -0.5438, -1.3986,  1.0015, -0.4257,  0.4278, -1.1540,\n",
       "        -0.2786,  1.5857])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-5a834387fb7d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-5a834387fb7d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    assert 1 == 0, raise RunTimeError\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "assert 1 == 0, raise RunTimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_500 = set()\n",
    "for sent in data_flat['en']:\n",
    "    for token in sent:\n",
    "        tokens_500.add(token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

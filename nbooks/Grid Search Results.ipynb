{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import join\n",
    "import pandas as pd\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from glob import glob\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpe_vs = ['500', '10000', '50000']\n",
    "word_vs = ['words-lower', 'words-mix']\n",
    "word_vocab_sizes = [5000, 10000, 25000]\n",
    "models = ['bilstm', 'gcn_head']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dicts(d1, d2):\n",
    "    res = deepcopy(d1)\n",
    "    for k, v in d2.items():\n",
    "        res[k] = v\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(f):\n",
    "    if isinstance(f, str) and os.path.exists(f):\n",
    "        with open(f) as f:\n",
    "            lines = [l.strip() for l in f.readlines()]\n",
    "    else:\n",
    "        lines = [l.strip() for l in f.readlines()]\n",
    "        \n",
    "    res = {}\n",
    "    buff = []\n",
    "    covered = []\n",
    "    keys = []\n",
    "    for line in lines:\n",
    "        line = line.split('|')[1]\n",
    "        if \"LR:\" in line:\n",
    "            if line not in covered:\n",
    "                covered.append(line)\n",
    "                key = line + \", AGGR: mean\"\n",
    "            else:\n",
    "                key = line + \", AGGR: max\"\n",
    "            if keys:\n",
    "                res[keys[-1]] = buff\n",
    "            keys.append(key)\n",
    "            buff = []\n",
    "        elif \"Epoch:\" in line:\n",
    "            buff.append(dict([param.strip().split(':') for param in line.split(',')]))\n",
    "    \n",
    "    res[key] = buff\n",
    "    \n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = parse_file(join(data_dir, f\"logs/gcn_head_vocab_10000_neg_sample_1_grid_search.log.log\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_pd_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_dict = {}\n",
    "\n",
    "for bpe_v in bpe_vs:\n",
    "    with open(join(data_dir, f\"grid_search/bilstm_vocab_{bpe_v}_neg_sample_1_grid_search.pkl\"), 'rb') as f:\n",
    "        bilstm_dict[bpe_v] = pickle.load(f)\n",
    "\n",
    "for word_v in word_vs:\n",
    "    for word_vocab_size in word_vocab_sizes:\n",
    "        with open(join(data_dir, f\"grid_search/bilstm_vocab_{word_v}_neg_sample_1_vocab_size_{word_vocab_size}_grid_search.pkl\"), 'rb') as f:\n",
    "            bilstm_dict[f'{word_v}|{word_vocab_size}'] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v, res_d in bilstm_dict.items():\n",
    "    if v.startswith('word'):\n",
    "        vocab, size = v.split(\"|\")\n",
    "        d = {'head_vocab': vocab, \"word_vocab_size\": size, \"side_vocabs\": \"NA\", \"AGGR\": \"NA\"}\n",
    "    else:\n",
    "        d = {'head_vocab': v, \"word_vocab_size\": -1, \"side_vocabs\": \"NA\", \"AGGR\": \"NA\"}\n",
    "    for key, losses in res_d.items():\n",
    "        d.update(dict([param.strip().split(':') for param in key.split(',')]))\n",
    "        for epoch, loss_d in enumerate(losses):\n",
    "            if not isinstance(loss_d, dict):\n",
    "                loss_d = {\"Accuracy\": loss_d, \"Train Loss\": -1, \"Valid Loss\": -1}\n",
    "            new_d = merge_dicts({'epoch': epoch, 'model': 'bilstm'}, loss_d)\n",
    "            bilstm_pd_list.append(merge_dicts(d, new_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7039,\n",
       " [0.9615293741226196,\n",
       "  0.9608191251754761,\n",
       "  0.9606415629386902,\n",
       "  0.9604048132896423,\n",
       "  0.9603456258773804,\n",
       "  0.9601088762283325,\n",
       "  0.9599905014038086,\n",
       "  0.9598721861839294,\n",
       "  0.9597538113594055,\n",
       "  0.9596354365348816])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs = sorted([float(rec['Accuracy'].numpy()) for rec in bilstm_pd_list], reverse=True)\n",
    "len(accs), accs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single head chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_pd_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_dict = {}\n",
    "\n",
    "for bpe_v in bpe_vs:\n",
    "    with open(join(data_dir, f\"logs/gcn_head_vocab_{bpe_v}_neg_sample_1_grid_search.log.log\"), 'r') as f:\n",
    "        gcn_dict[bpe_v] = parse_file(f)\n",
    "\n",
    "for word_v in word_vs:\n",
    "    for word_vocab_size in word_vocab_sizes:\n",
    "        with open(join(data_dir, f\"logs/gcn_head_vocab_{word_v}_neg_sample_1_vocab_size_{word_vocab_size}_grid_search.log.log\"), 'r') as f:\n",
    "            gcn_dict[f'{word_v}|{word_vocab_size}'] = parse_file(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v, res_d in gcn_dict.items():\n",
    "    if v.startswith('word'):\n",
    "        vocab, size = v.split(\"|\")\n",
    "        d = {'head_vocab': vocab, \"word_vocab_size\": size, \"side_vocabs\": \"NA\"}\n",
    "    else:\n",
    "        d = {'head_vocab': v, \"word_vocab_size\": -1, \"side_vocabs\": \"NA\"}\n",
    "    for key, losses in res_d.items():\n",
    "        d.update(dict([param.strip().split(':') for param in key.split(',')]))\n",
    "        for epoch, loss_d in enumerate(losses):\n",
    "            new_d = merge_dicts({'epoch': epoch, 'model': 'gcn'}, loss_d)\n",
    "            gcn_pd_list.append(merge_dicts(d, new_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['500', '10000', '50000', 'words-lower|5000', 'words-lower|10000', 'words-lower|25000', 'words-mix|5000', 'words-mix|10000', 'words-mix|25000'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2880,\n",
       " [0.961,\n",
       "  0.9601,\n",
       "  0.9596,\n",
       "  0.9595,\n",
       "  0.9592,\n",
       "  0.9592,\n",
       "  0.9587,\n",
       "  0.9585,\n",
       "  0.9584,\n",
       "  0.958])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs = sorted([float(rec['Accuracy'].strip()) for rec in gcn_pd_list], reverse=True)\n",
    "len(accs), accs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 head and 1 BPE Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_dict = {}\n",
    "\n",
    "for word_v in word_vs:\n",
    "    for bpe_v in bpe_vs:\n",
    "        for word_vocab_size in word_vocab_sizes:\n",
    "            with open(join(data_dir, f\"logs/gcn_head_vocab_{word_v}_bpe_vocab_{bpe_v}_neg_sample_1_vocab_size_{word_vocab_size}_grid_search.log.log\"), 'r') as f:\n",
    "                gcn_dict[f'{word_v}|{bpe_v}|{word_vocab_size}'] = parse_file(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v, res_d in gcn_dict.items():\n",
    "    word_v, bpe_v, size = v.split(\"|\")\n",
    "    d = {'head_vocab': vocab, \"word_vocab_size\": size, \"side_vocabs\": bpe_v}\n",
    "    for key, losses in res_d.items():\n",
    "        d.update(dict([param.strip().split(':') for param in key.split(',')]))\n",
    "        for epoch, loss_d in enumerate(losses):\n",
    "            new_d = merge_dicts({'epoch': epoch, 'model': 'gcn'}, loss_d)\n",
    "            gcn_pd_list.append(merge_dicts(d, new_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8640,\n",
       " [0.961,\n",
       "  0.9601,\n",
       "  0.9596,\n",
       "  0.9595,\n",
       "  0.9592,\n",
       "  0.9592,\n",
       "  0.9587,\n",
       "  0.9585,\n",
       "  0.9584,\n",
       "  0.958])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs = sorted([float(rec['Accuracy'].strip()) for rec in gcn_pd_list], reverse=True)\n",
    "len(accs), accs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 head and multiple BPE Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_names = glob(join(data_dir, f\"logs/gcn_head*bpe_vocabs*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_dict = {}\n",
    "reg = re.compile(r\".*gcn_head_vocab_(.*)_bpe_vocabs_(.*)_neg_sample_1_vocab_size_(.*)_grid_search\")\n",
    "for f_name in f_names:\n",
    "    with open(f_name, \"r\") as f:\n",
    "        res_d = parse_file(f)\n",
    "    m = reg.match(f_name)\n",
    "    \n",
    "    word_vocab = m.group(1)\n",
    "    bpe_vocabs = m.group(2)\n",
    "    word_vocab_size = m.group(3)\n",
    "    d = {'head_vocab': word_vocab, \"word_vocab_size\": word_vocab_size, \"side_vocabs\": bpe_vocabs}\n",
    "    for key, losses in res_d.items():\n",
    "        d.update(dict([param.strip().split(':') for param in key.split(',')]))\n",
    "        for epoch, loss_d in enumerate(losses):\n",
    "            new_d = merge_dicts({'epoch': epoch, 'model': 'gcn'}, loss_d)\n",
    "            gcn_pd_list.append(merge_dicts(d, new_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16320,\n",
       " [0.9614,\n",
       "  0.961,\n",
       "  0.9601,\n",
       "  0.9596,\n",
       "  0.9595,\n",
       "  0.9592,\n",
       "  0.9592,\n",
       "  0.9587,\n",
       "  0.9585,\n",
       "  0.9584])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs = sorted([float(rec['Accuracy'].strip()) for rec in gcn_pd_list], reverse=True)\n",
    "len(accs), accs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(bilstm_pd_list + gcn_pd_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGGR</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>DP</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>LR</th>\n",
       "      <th>NUM_LAYERS</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Valid Loss</th>\n",
       "      <th>WD</th>\n",
       "      <th>epoch</th>\n",
       "      <th>head_vocab</th>\n",
       "      <th>model</th>\n",
       "      <th>side_vocabs</th>\n",
       "      <th>word_vocab_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NA</td>\n",
       "      <td>tensor(0.5203)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.678797</td>\n",
       "      <td>0.710672</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0</td>\n",
       "      <td>500</td>\n",
       "      <td>bilstm</td>\n",
       "      <td>NA</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NA</td>\n",
       "      <td>tensor(0.4940)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.667404</td>\n",
       "      <td>0.684273</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "      <td>bilstm</td>\n",
       "      <td>NA</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NA</td>\n",
       "      <td>tensor(0.5798)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.653367</td>\n",
       "      <td>0.651956</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>bilstm</td>\n",
       "      <td>NA</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NA</td>\n",
       "      <td>tensor(0.5810)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.649096</td>\n",
       "      <td>0.649752</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>bilstm</td>\n",
       "      <td>NA</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NA</td>\n",
       "      <td>tensor(0.5790)</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>2</td>\n",
       "      <td>0.654959</td>\n",
       "      <td>0.676428</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>4</td>\n",
       "      <td>500</td>\n",
       "      <td>bilstm</td>\n",
       "      <td>NA</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  AGGR        Accuracy    DP Epoch     LR NUM_LAYERS Train Loss Valid Loss  \\\n",
       "0   NA  tensor(0.5203)   0.3   NaN   0.01          2   0.678797   0.710672   \n",
       "1   NA  tensor(0.4940)   0.3   NaN   0.01          2   0.667404   0.684273   \n",
       "2   NA  tensor(0.5798)   0.3   NaN   0.01          2   0.653367   0.651956   \n",
       "3   NA  tensor(0.5810)   0.3   NaN   0.01          2   0.649096   0.649752   \n",
       "4   NA  tensor(0.5790)   0.3   NaN   0.01          2   0.654959   0.676428   \n",
       "\n",
       "        WD  epoch head_vocab   model side_vocabs word_vocab_size  \n",
       "0   0.0005      0        500  bilstm          NA              -1  \n",
       "1   0.0005      1        500  bilstm          NA              -1  \n",
       "2   0.0005      2        500  bilstm          NA              -1  \n",
       "3   0.0005      3        500  bilstm          NA              -1  \n",
       "4   0.0005      4        500  bilstm          NA              -1  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df ['Accuracy'] = df['Accuracy'].apply(lambda x: x.numpy() if isinstance(x, torch.Tensor) else float(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "bilstm    0.961529\n",
       "gcn       0.961400\n",
       "Name: Accuracy, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['model']).max()['Accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gcn = df[df[\"model\"] == \"gcn\"]\n",
    "df_bilstm = df[df[\"model\"] == \"bilstm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>side_vocabs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>head_vocab</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>0.957741</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.961529</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50000</th>\n",
       "      <td>0.837955</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words-lower</th>\n",
       "      <td>0.961400</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>words-mix</th>\n",
       "      <td>0.960100</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Accuracy side_vocabs\n",
       "head_vocab                       \n",
       "10000        0.957741          NA\n",
       "500          0.961529          NA\n",
       "50000        0.837955          NA\n",
       "words-lower  0.961400          NA\n",
       "words-mix    0.960100          NA"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['head_vocab']).max()[['Accuracy', 'side_vocabs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NA', '500', '10000', '50000', '50000-10000-500', '50000-10000',\n",
       "       '500-10000', '500-50000'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['side_vocabs'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "side_vocabs\n",
       "10000              0.956200\n",
       "500                0.952900\n",
       "500-10000          0.954500\n",
       "500-50000          0.961400\n",
       "50000              0.957700\n",
       "50000-10000        0.954700\n",
       "50000-10000-500    0.953400\n",
       "NA                 0.961529\n",
       "Name: Accuracy, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['side_vocabs']).max()['Accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>DP</th>\n",
       "      <th>LR</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Valid Loss</th>\n",
       "      <th>WD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.056601</td>\n",
       "      <td>-0.331849</td>\n",
       "      <td>-0.165755</td>\n",
       "      <td>-0.173204</td>\n",
       "      <td>-0.207216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DP</th>\n",
       "      <td>-0.056601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174094</td>\n",
       "      <td>0.022371</td>\n",
       "      <td>0.027693</td>\n",
       "      <td>0.115207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LR</th>\n",
       "      <td>-0.331849</td>\n",
       "      <td>0.174094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.197108</td>\n",
       "      <td>0.216322</td>\n",
       "      <td>0.193904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train Loss</th>\n",
       "      <td>-0.165755</td>\n",
       "      <td>0.022371</td>\n",
       "      <td>0.197108</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.941466</td>\n",
       "      <td>0.119574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Valid Loss</th>\n",
       "      <td>-0.173204</td>\n",
       "      <td>0.027693</td>\n",
       "      <td>0.216322</td>\n",
       "      <td>0.941466</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.128708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WD</th>\n",
       "      <td>-0.207216</td>\n",
       "      <td>0.115207</td>\n",
       "      <td>0.193904</td>\n",
       "      <td>0.119574</td>\n",
       "      <td>0.128708</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Accuracy        DP        LR  Train Loss  Valid Loss        WD\n",
       "Accuracy    1.000000 -0.056601 -0.331849   -0.165755   -0.173204 -0.207216\n",
       "DP         -0.056601  1.000000  0.174094    0.022371    0.027693  0.115207\n",
       "LR         -0.331849  0.174094  1.000000    0.197108    0.216322  0.193904\n",
       "Train Loss -0.165755  0.022371  0.197108    1.000000    0.941466  0.119574\n",
       "Valid Loss -0.173204  0.027693  0.216322    0.941466    1.000000  0.128708\n",
       "WD         -0.207216  0.115207  0.193904    0.119574    0.128708  1.000000"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['Accuracy', 'DP', 'LR', 'Train Loss', 'Valid Loss', 'WD']].astype('float32').corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_names = glob(join(data_dir, \"grid_search/model_gcn*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_None_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_50000,10000_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_None_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_500,50000_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_500,10000,50000_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_500_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_10000,500_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_10000,50000_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_10000_side_vocabs_None_word_vocab_size_None.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_500,10000_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_10000,50000_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_50000,500_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_500,10000_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_50000_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_500_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_500,10000_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_None_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_10000_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_10000,50000_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_500_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_10000,500_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_500,10000,50000_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_10000_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_50000,500_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_None_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_50000_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_500,10000,50000_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_50000_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_10000,500_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_50000,10000_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_50000,500_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_10000_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_500,10000,50000_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_500_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_10000,50000_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_500_side_vocabs_None_word_vocab_size_None.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_500,50000_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_50000_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_500_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_50000,10000_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_50000_side_vocabs_None_word_vocab_size_None.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_None_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_10000,50000_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_10000_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_500,10000,50000_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_None_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_50000,10000_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_10000,500_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_10000_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_50000,500_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_10000,500_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_50000_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_50000,10000_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_50000,10000_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_500,10000_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_50000_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_500_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_10000,500_word_vocab_size_10000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_50000,500_word_vocab_size_5000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-lower_side_vocabs_50000,500_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_10000_word_vocab_size_25000.pkl',\n",
       " '/nfs/team/nlp/users/rgupta/NMT/code/GNN-Semantic-Similarity/local/grid_search/model_gcn_main_vocab_words-mix_side_vocabs_10000,50000_word_vocab_size_5000.pkl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "for f_name in f_names:\n",
    "    with open(f_name, \"rb\") as f:\n",
    "        records.extend(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3460"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accuracy              0.527344\n",
       "DP                         0.3\n",
       "Epoch                        8\n",
       "LR                       0.005\n",
       "NUM_LAYERS                   2\n",
       "Train Loss            0.597887\n",
       "Valid Loss            0.656703\n",
       "WD                      0.0005\n",
       "main_vocab         words-lower\n",
       "model                      gcn\n",
       "side_vocabs                500\n",
       "word_vocab_size          25000\n",
       "Name: 338, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[338]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338     0.527344\n",
       "339     0.527344\n",
       "1698    0.496094\n",
       "996     0.492188\n",
       "998     0.492188\n",
       "119     0.484375\n",
       "1669    0.480469\n",
       "2779    0.480469\n",
       "3369    0.472656\n",
       "1489    0.468750\n",
       "2549    0.468750\n",
       "1037    0.460938\n",
       "938     0.460938\n",
       "3368    0.460938\n",
       "909     0.457031\n",
       "2539    0.457031\n",
       "918     0.457031\n",
       "248     0.453125\n",
       "337     0.453125\n",
       "1699    0.449219\n",
       "2489    0.449219\n",
       "1019    0.449219\n",
       "2939    0.445312\n",
       "118     0.445312\n",
       "2399    0.445312\n",
       "978     0.445312\n",
       "929     0.445312\n",
       "937     0.445312\n",
       "2537    0.441406\n",
       "78      0.441406\n",
       "          ...   \n",
       "1465    0.378906\n",
       "1716    0.378906\n",
       "1797    0.378906\n",
       "1957    0.378906\n",
       "735     0.378906\n",
       "2818    0.378906\n",
       "2817    0.378906\n",
       "2319    0.378906\n",
       "315     0.375000\n",
       "399     0.375000\n",
       "2819    0.375000\n",
       "1799    0.375000\n",
       "437     0.375000\n",
       "716     0.375000\n",
       "2418    0.375000\n",
       "1798    0.375000\n",
       "1837    0.375000\n",
       "1958    0.375000\n",
       "395     0.371094\n",
       "816     0.371094\n",
       "3327    0.371094\n",
       "48      0.371094\n",
       "818     0.371094\n",
       "2433    0.371094\n",
       "3308    0.367188\n",
       "3307    0.367188\n",
       "2653    0.367188\n",
       "757     0.367188\n",
       "715     0.367188\n",
       "756     0.355469\n",
       "Name: Accuracy, Length: 3460, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Accuracy'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
